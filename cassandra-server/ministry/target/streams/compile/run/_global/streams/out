[0m[[0m[31merror[0m] [0m[0morg.apache.spark.sql.streaming.StreamingQueryException: Multiple streaming queries are concurrently using file:/C:/Projects/Saarthi-Camps/cassandra-server/ministry/chk-point-dir/offsets/3[0m
[0m[[0m[31merror[0m] [0m[0m=== Streaming Query ===[0m
[0m[[0m[31merror[0m] [0m[0mIdentifier: Flattened Invoice Writer [id = 07692540-d988-475c-9297-b4107955164f, runId = c9882062-699b-4601-b37d-516e63ed7d4f][0m
[0m[[0m[31merror[0m] [0m[0mCurrent Committed Offsets: {KafkaV2[Subscribe[CampInfo]]: {"CampInfo":{"0":2}}}[0m
[0m[[0m[31merror[0m] [0m[0mCurrent Available Offsets: {KafkaV2[Subscribe[CampInfo]]: {"CampInfo":{"0":3}}}[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mCurrent State: ACTIVE[0m
[0m[[0m[31merror[0m] [0m[0mThread State: RUNNABLE[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mLogical Plan:[0m
[0m[[0m[31merror[0m] [0m[0mProject [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, ItemCode#47, ItemDescription#61, ItemPrice#76, ItemQty#92, TotalValue#109][0m
[0m[[0m[31merror[0m] [0m[0m+- Project [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, LineItem#35, ItemCode#47, ItemDescription#61, ItemPrice#76, ItemQty#92, LineItem#35.TotalValue AS TotalValue#109][0m
[0m[[0m[31merror[0m] [0m[0m   +- Project [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, LineItem#35, ItemCode#47, ItemDescription#61, ItemPrice#76, LineItem#35.ItemQty AS ItemQty#92][0m
[0m[[0m[31merror[0m] [0m[0m      +- Project [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, LineItem#35, ItemCode#47, ItemDescription#61, LineItem#35.ItemPrice AS ItemPrice#76][0m
[0m[[0m[31merror[0m] [0m[0m         +- Project [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, LineItem#35, ItemCode#47, LineItem#35.ItemDescription AS ItemDescription#61][0m
[0m[[0m[31merror[0m] [0m[0m            +- Project [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, LineItem#35, LineItem#35.ItemCode AS ItemCode#47][0m
[0m[[0m[31merror[0m] [0m[0m               +- Project [value#21.InvoiceNumber AS InvoiceNumber#24, value#21.CreatedTime AS CreatedTime#25L, value#21.StoreID AS StoreID#26, value#21.PosID AS PosID#27, value#21.CustomerType AS CustomerType#28, value#21.PaymentMethod AS PaymentMethod#29, value#21.DeliveryType AS DeliveryType#30, value#21.DeliveryAddress.City AS City#31, value#21.DeliveryAddress.State AS State#32, value#21.DeliveryAddress.PinCode AS PinCode#33, LineItem#35][0m
[0m[[0m[31merror[0m] [0m[0m                  +- Generate explode(value#21.InvoiceLineItems), false, [LineItem#35][0m
[0m[[0m[31merror[0m] [0m[0m                     +- Project [from_json(StructField(InvoiceNumber,StringType,true), StructField(CreatedTime,LongType,true), StructField(StoreID,StringType,true), StructField(PosID,StringType,true), StructField(CashierID,StringType,true), StructField(CustomerType,StringType,true), StructField(CustomerCardNo,StringType,true), StructField(TotalAmount,DoubleType,true), StructField(NumberOfItems,IntegerType,true), StructField(PaymentMethod,StringType,true), StructField(CGST,DoubleType,true), StructField(SGST,DoubleType,true), StructField(CESS,DoubleType,true), StructField(DeliveryType,StringType,true), StructField(DeliveryAddress,StructType(StructField(AddressLine,StringType,true), StructField(City,StringType,true), StructField(State,StringType,true), StructField(PinCode,StringType,true), StructField(ContactNumber,StringType,true)),true), StructField(InvoiceLineItems,ArrayType(StructType(StructField(ItemCode,StringType,true), StructField(ItemDescription,StringType,true), StructField(ItemPrice,DoubleType,true), StructField(ItemQty,IntegerType,true), StructField(TotalValue,DoubleType,true)),true),true), cast(value#8 as string), Some(Asia/Calcutta)) AS value#21][0m
[0m[[0m[31merror[0m] [0m[0m                        +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@2f39bf1c, KafkaV2[Subscribe[CampInfo]][0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:355)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.util.ConcurrentModificationException: Multiple streaming queries are concurrently using file:/C:/Projects/Saarthi-Camps/cassandra-server/ministry/chk-point-dir/offsets/3[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.writeBatchToFile(HDFSMetadataLog.scala:141)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$add$3(HDFSMetadataLog.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:189)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:118)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$12(MicroBatchExecution.scala:418)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: org.apache.hadoop.fs.FileAlreadyExistsException: Rename destination file:/C:/Projects/Saarthi-Camps/cassandra-server/ministry/chk-point-dir/offsets/3 already exists.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:740)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:240)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:690)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.FileContext.rename(FileContext.java:958)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:329)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:147)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.writeBatchToFile(HDFSMetadataLog.scala:134)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$add$3(HDFSMetadataLog.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:189)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:118)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$12(MicroBatchExecution.scala:418)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:416)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) org.apache.spark.sql.streaming.StreamingQueryException: Multiple streaming queries are concurrently using file:/C:/Projects/Saarthi-Camps/cassandra-server/ministry/chk-point-dir/offsets/3[0m
[0m[[0m[31merror[0m] [0m[0m=== Streaming Query ===[0m
[0m[[0m[31merror[0m] [0m[0mIdentifier: Flattened Invoice Writer [id = 07692540-d988-475c-9297-b4107955164f, runId = c9882062-699b-4601-b37d-516e63ed7d4f][0m
[0m[[0m[31merror[0m] [0m[0mCurrent Committed Offsets: {KafkaV2[Subscribe[CampInfo]]: {"CampInfo":{"0":2}}}[0m
[0m[[0m[31merror[0m] [0m[0mCurrent Available Offsets: {KafkaV2[Subscribe[CampInfo]]: {"CampInfo":{"0":3}}}[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mCurrent State: ACTIVE[0m
[0m[[0m[31merror[0m] [0m[0mThread State: RUNNABLE[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mLogical Plan:[0m
[0m[[0m[31merror[0m] [0m[0mProject [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, ItemCode#47, ItemDescription#61, ItemPrice#76, ItemQty#92, TotalValue#109][0m
[0m[[0m[31merror[0m] [0m[0m+- Project [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, LineItem#35, ItemCode#47, ItemDescription#61, ItemPrice#76, ItemQty#92, LineItem#35.TotalValue AS TotalValue#109][0m
[0m[[0m[31merror[0m] [0m[0m   +- Project [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, LineItem#35, ItemCode#47, ItemDescription#61, ItemPrice#76, LineItem#35.ItemQty AS ItemQty#92][0m
[0m[[0m[31merror[0m] [0m[0m      +- Project [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, LineItem#35, ItemCode#47, ItemDescription#61, LineItem#35.ItemPrice AS ItemPrice#76][0m
[0m[[0m[31merror[0m] [0m[0m         +- Project [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, LineItem#35, ItemCode#47, LineItem#35.ItemDescription AS ItemDescription#61][0m
[0m[[0m[31merror[0m] [0m[0m            +- Project [InvoiceNumber#24, CreatedTime#25L, StoreID#26, PosID#27, CustomerType#28, PaymentMethod#29, DeliveryType#30, City#31, State#32, PinCode#33, LineItem#35, LineItem#35.ItemCode AS ItemCode#47][0m
[0m[[0m[31merror[0m] [0m[0m               +- Project [value#21.InvoiceNumber AS InvoiceNumber#24, value#21.CreatedTime AS CreatedTime#25L, value#21.StoreID AS StoreID#26, value#21.PosID AS PosID#27, value#21.CustomerType AS CustomerType#28, value#21.PaymentMethod AS PaymentMethod#29, value#21.DeliveryType AS DeliveryType#30, value#21.DeliveryAddress.City AS City#31, value#21.DeliveryAddress.State AS State#32, value#21.DeliveryAddress.PinCode AS PinCode#33, LineItem#35][0m
[0m[[0m[31merror[0m] [0m[0m                  +- Generate explode(value#21.InvoiceLineItems), false, [LineItem#35][0m
[0m[[0m[31merror[0m] [0m[0m                     +- Project [from_json(StructField(InvoiceNumber,StringType,true), StructField(CreatedTime,LongType,true), StructField(StoreID,StringType,true), StructField(PosID,StringType,true), StructField(CashierID,StringType,true), StructField(CustomerType,StringType,true), StructField(CustomerCardNo,StringType,true), StructField(TotalAmount,DoubleType,true), StructField(NumberOfItems,IntegerType,true), StructField(PaymentMethod,StringType,true), StructField(CGST,DoubleType,true), StructField(SGST,DoubleType,true), StructField(CESS,DoubleType,true), StructField(DeliveryType,StringType,true), StructField(DeliveryAddress,StructType(StructField(AddressLine,StringType,true), StructField(City,StringType,true), StructField(State,StringType,true), StructField(PinCode,StringType,true), StructField(ContactNumber,StringType,true)),true), StructField(InvoiceLineItems,ArrayType(StructType(StructField(ItemCode,StringType,true), StructField(ItemDescription,StringType,true), StructField(ItemPrice,DoubleType,true), StructField(ItemQty,IntegerType,true), StructField(TotalValue,DoubleType,true)),true),true), cast(value#8 as string), Some(Asia/Calcutta)) AS value#21][0m
[0m[[0m[31merror[0m] [0m[0m                        +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@2f39bf1c, KafkaV2[Subscribe[CampInfo]][0m
