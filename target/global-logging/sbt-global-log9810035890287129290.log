[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Processing event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/Saarthi-Camps/cassandra-server/ministry/src/main/scala/Main.scala","languageId":"scala","version":1,"text":"import org.apache.log4j.Logger\r\nimport org.apache.spark.sql.SparkSession\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types._\r\nimport org.apache.spark.sql.streaming.Trigger\r\nimport org.apache.spark.sql.streaming.StreamingQuery\r\nimport org.apache.spark.sql.streaming.StreamingQueryListener\r\nimport org.apache.spark.sql.streaming.StreamingQueryListener.QueryProgressEvent\r\nimport org.apache.spark.sql.streaming.StreamingQueryListener.QueryStartedEvent\r\nimport org.apache.log4j.Level\r\n\r\n\r\nobject main extends App {\r\n  @transient lazy val logger: Logger = Logger.getLogger(getClass.getName)\r\n\r\n  // System.setProperty(\"log4j.configuration\", \"C:/Projects/Saarthi-Camps/cassandra-server/ministry/log4j.properties\")\r\n\r\n  // org.apache.log4j.Logger.getLogger(\"org.apache.kafka.clients.consumer.Consumer\")\r\n  // .setLevel(Level.ERROR)\r\n\r\n  val spark = SparkSession.builder()\r\n      .master(\"local[3]\")\r\n      .appName(\"enrichment\")\r\n      .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\r\n      .config(\"spark.sql.shuffle.partitions\", 2)\r\n      .config(\"spark.cassandra.connection.host\", \"localhost\")\r\n      .config(\"spark.cassandra.connection.port\", \"9042\")\r\n      .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\r\n      .config(\"spark.sql.catalog.lh\", \"com.datastax.spark.connector.datasource.CassandraCatalog\")\r\n      .getOrCreate()\r\n\r\n  spark.sparkContext.setLogLevel(\"ERROR\")\r\n  import spark.implicits._\r\n  \r\n  val kafkaStreamDF = spark\r\n    .readStream\r\n    .format(\"kafka\")\r\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\r\n    .option(\"subscribe\", \"CampInfo\")\r\n    .load()\r\n\r\n  val schema = StructType(Seq(\r\n    StructField(\"id\", StringType, nullable = false),\r\n    StructField(\"Medicines\", MapType(StringType, IntegerType), nullable = false),\r\n    StructField(\"Urgency\", IntegerType, nullable = false)\r\n  ))\r\n  val jsonStreamDF = kafkaStreamDF\r\n    .selectExpr(\"CAST(value AS STRING)\")\r\n    .select(from_json($\"value\", schema).as(\"data\"))\r\n    .select(\"data.*\")\r\n  val aggDF = jsonStreamDF\r\n    .groupBy($\"id\")\r\n    .agg(\r\n      map_concat(collect_list($\"Medicines\"), lit(\" \")).as(\"MergedMedicines\"),\r\n      max($\"Urgency\").as(\"MaxUrgency\")\r\n    )\r\n\r\n  val invoiceWriterQuery = aggDF.writeStream\r\n    .outputMode(\"append\")\r\n    .foreachBatch((batchDF: org.apache.spark.sql.DataFrame, batchId: Long) => {\r\n      //val df = spark.createDataFrame(spark.sparkContext.parallelize(rows), schema)\r\n      \r\n      val stationsDB = spark.read\r\n        .format(\"org.apache.spark.sql.cassandra\")\r\n        .option(\"keyspace\", \"camp_db\")\r\n        .option(\"table\", \"stations\")\r\n        .load()\r\n\r\n      val aggDF = batchDF.select()\r\n\r\n      \r\n      //batchDF.show()\r\n      // batchDF.write\r\n      //   .format(\"org.apache.spark.sql.cassandra\")\r\n      //   .options(Map(\"keyspace\" -> \"camp_db\", \"table\" -> \"stations\")) \r\n      //   .mode(\"append\")\r\n      //   .save()\r\n\r\n\r\n    })\r\n    .trigger(Trigger.ProcessingTime(\"1 minute\"))\r\n    .start()\r\n  \r\n  logger.info(\"Listening to Kafka\")\r\n  invoiceWriterQuery.awaitTermination()\r\n\r\n\r\n}\r\n\r\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (C:\Projects\Saarthi-Camps\target\scala-2.12\zinc\inc_compile_2.12.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 0 s, completed 29-Oct-2023, 2:11:51 PM[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Done event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled request received: shutdown: JsonRpcRequestMessage(2.0, â™¨1, shutdown, null})[0m
